The data obtained from this paper:
https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-021-00852-8#availability-of-data-and-materials
https://www.ncbi.nlm.nih.gov/sra/?term=PRJNA681391
There is 107 samples and 214 

1.	Download the fastq from NCBI db
fastqdump.sh
You can use split to split the file before submit the job
#split -l 100 -d list.txt myfile
#split -l 2 -d list_srr_human.txt split_ # splited file include only 2 lines

#!/bin/bash
cd /data/duann2/human_proj/qsubfile/
for f in `cat one.txt` 
#one.txt:
#SRR13215448
#SRR13215449
...

do
cat <<EOT >> $f\.qsub 
#!/bin/bash
#SBATCH --partition=norm
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --mem=32g
#SBATCH --gres=lscratch:500
#SBATCH --time=5-00:00:00
module load sratoolkit
prefetch ${f} --max-size 200g # you gonna get a bunch of .sra file 
#or you can use a bulk downloading(srrlist.txt contain the list of sample name): prefetch --option-file /data/duann2/srrlist.txt --max-size 100g -O /data/duann2/human_proj

fastq-dump --outdir /data/duann2/human_proj/fastq12 --gzip --readids --split-files /data/duann2/human_proj/qsubfile/${f}/${f}.sra # you will get 2 fastq file

EOT
sbatch /data/duann2/human_proj/qsubfile/$f\.qsub #submit all these jobs and run it
done

2. Downloading the human db reference GRCh38.p13 to do the bowtie mapping
module load edirect
download ftp link and save it in the file list_huref_test.txt
#!/bin/bash
esearch -db bioproject -query 31257 \
        | elink -target assembly \
        | esummary \
        | grep "FtpPath_GenBank"\
        | sed -e 's/<*FtpPath_GenBank>//g'\
        | sed -e 's/<//g'\
        | sed -r 's|.+>(ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA_.+)<.+|\1|'\
        > list_huref_test.txt

list_huref.txt :
ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.28_GRCh38.p13/

wget -r -R "index.html" --no-host-directories --cut-dirs=6 -i list_huref.txt

3. bowtie2 db building
bowtie2-build -f /data/EmiolaLab/duann2/human_proj/human_ref/GCA_000001405.28_GRCh38.p13_genomic.fna human_index 
#index the reference genome,this doesn't work, might because contain to many Ns, So I download the index file directly from the website
ftp:///ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/

4. bowtie mapping
bowtie db looks like this:
-rw-r-----. 1 duann2 EmiolaLab 970M Nov 19  2014 GCA_000001405.15_GRCh38_full_analysis_set.fna.bowtie_index.1.bt2
-rw-r-----. 1 duann2 EmiolaLab 724M Nov 19  2014 GCA_000001405.15_GRCh38_full_analysis_set.fna.bowtie_index.2.bt2
-rw-r-----. 1 duann2 EmiolaLab  15K Nov 19  2014 GCA_000001405.15_GRCh38_full_analysis_set.fna.bowtie_index.3.bt2
-rw-r-----. 1 duann2 EmiolaLab 724M Nov 19  2014 GCA_000001405.15_GRCh38_full_analysis_set.fna.bowtie_index.4.bt2
-rw-r-----. 1 duann2 EmiolaLab 970M Nov 19  2014 GCA_000001405.15_GRCh38_full_analysis_set.fna.bowtie_index.rev.1.bt2
-rw-r-----. 1 duann2 EmiolaLab 724M Nov 19  2014 GCA_000001405.15_GRCh38_full_analysis_set.fna.bowtie_index.rev.2.bt2

#!/bin/bash
cd /data/EmiolaLab/duann2/human_proj/human_ref/qsubfile
for f in `cat fix.txt`
do
cat <<EOT >> $f\.qsub
#!/bin/bash
#SBATCH --partition=norm
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16 
#SBATCH --mem=100g
#SBATCH --gres=lscratch:800
#SBATCH --time=8-12:00:00
module load bowtie/2
module load bedtools
module load samtools
#!/bin/bash
cd /data/EmiolaLab/duann2/human_proj/human_ref/qsubfile

bowtie2 -x GCA_000001405.15_GRCh38_full_analysis_set.fna.bowtie_index -p 16 -1 /data/duann2/human_proj/repair_fastq/${f}_1_paired.fq.gz -2 /data/duann2/human_proj/repair_fastq/${f}_2_paired.fq.gz -S /data/EmiolaLab/duann2/human_proj/human_ref/samfile/${f}.sam
# this is pair end
#bowtie2 -x GCA_000001405.15_GRCh38_full_analysis_set.fna.bowtie_index -p 16 -f -U /data/EmiolaLab/Oral/Periodontitis/${f}.fasta.gz -S /data/EmiolaLab/duann2/sgene_proj/smorf_mapping/samfile/${f}.sam
# this is for single end, you give -f must be fasta file!!!

samtools view -bS -@ 16 /data/EmiolaLab/duann2/human_proj/human_ref/samfile/${f}.sam | samtools sort > /data/EmiolaLab/duann2/human_proj/human_ref/samfile/${f}_sortedIndexed.bam

rm /data/EmiolaLab/duann2/human_proj/human_ref/samfile/${f}.sam #too big, so I usually delete it

samtools index /data/EmiolaLab/duann2/human_proj/human_ref/samfile/${f}_sortedIndexed.bam

bedtools genomecov -ibam /data/EmiolaLab/duann2/human_proj/human_ref/samfile/${f}_sortedIndexed.bam -d > /data/EmiolaLab/duann2/human_proj/human_ref/samfile/${f}_sortedIndexed.bam.tsv
#give you the coverage on each nucleatide location

EOT
sbatch /data/EmiolaLab/duann2/human_proj/human_ref/qsubfile/$f\.qsub
done

# # batch job list in list example (two for loop)
#!/bin/bash
cd /data/EmiolaLab/duann2/human_proj/human_ref/qsubfile
for f in human.* #split sample into different files
do
cat <<EOT >> $f\.qsub
#!/bin/bash
#SBATCH --partition=norm
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16 
#SBATCH --mem=32g
#SBATCH --gres=lscratch:500
#SBATCH --time=72:00:00
module load bowtie/2
module load bedtools
module load samtools
#!/bin/bash
cd /data/EmiolaLab/duann2/human_proj/human_ref/
for i in \`cat $f\`
do
bowtie2 -x GCA_000001405.15_GRCh38_full_analysis_set.fna.bowtie_index -1 /data/duann2/human_proj/fastq/\$i\_pass_1.fastq.gz -2 /data/duann2/human_proj/fastq/\$i\_pass_2.fastq.gz -S /data/EmiolaLab/duann2/human_proj/human_ref/samfile/\$i\.sam
samtools view -bS -@ 16 \$i\.sam | samtools sort > \$i\_sortedIndexed.bam #sometimes you will get an error due to the ram
samtools index \$i\_sortedIndexed.bam

bedtools genomecov -ibam \$i\_sortedIndexed.bam -bga -max 10 > \$i\_sortedIndexed.bam.tsv
done
EOT
sbatch /data/EmiolaLab/duann2/human_proj/human_ref/qsubfile/$f\.qsub
done

!!!There a error when mapping due to the uneven number of reads in two paired fastq file, it is possibly because that raw file brokend or they trim the two fastq file seperately
#####to repair the fastq file that have uneven number of reads, so I use repair.sh(in bbmap, bbtools)########
repair.sh in1=/data/duann2/human_proj/fastq/${f}_1.fastq.gz in2=/data/duann2/human_proj/fastq/${f}_2.fastq.gz out1=/data/EmiolaLab/duann2/human_proj/human_ref/fastq/${f}_fixed_1.fastq.gz out2=/data/EmiolaLab/duann2/human_proj/human_ref/fastq/${f}_fixed_2.fastq.gz outs=/data/EmiolaLab/duann2/human_proj/human_ref/fastq/${f}_singletons.fq repair

bbtools repair in1=/data/duann2/human_proj/fastq/SRR13215448_1.fastq.gz in2=/data/duann2/human_proj/fastq/SRR13215448_2.fastq.gz out1=/data/EmiolaLab/duann2/human_proj/human_ref/fastq/SRR13215448_fixed_1.fastq.gz out2=/data/EmiolaLab/duann2/human_proj/human_ref/fastq/SRR13215448_fixed_2.fastq.gz outs=/data/EmiolaLab/duann2/human_proj/human_ref/fastq/SRR13215448_singletons.fq -Xmx100g repair
#########it seems like doesn't work, so I am using trimmomatic to run it########
module load trimmomatic
java -jar $TRIMMOJAR PE -phred33 -threads 16 /data/duann2/human_proj/fastq/SRR13215448_1.fastq.gz /data/duann2/human_proj/fastq/SRR13215448_2.fastq.gz \
output_forward_paired.fq.gz output_forward_unpaired.fq.gz \
output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz \
ILLUMINACLIP:/usr/local/apps/trimmomatic/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36

java -jar $TRIMMOJAR PE -phred33 -threads 16 /data/duann2/human_proj/fastq/SRR13215457_1.fastq.gz /data/duann2/human_proj/fastq/SRR13215457_2.fastq.gz\
 output_forward_paired.fq.gz output_forward_unpaired.fq.gz output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz \
ILLUMINACLIP:/usr/local/apps/trimmomatic/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36

##############trimmomatic##########################
#!/bin/bash
cd /data/duann2/human_proj/repair_fastq/
for f in `cat repair.txt`
do
cat <<EOT >> $f\.qsub
#!/bin/bash
#SBATCH --partition=norm
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --mem=200g
#SBATCH --gres=lscratch:500
#SBATCH --time=5-00:00:00
module load trimmomatic

java -jar $TRIMMOJAR PE -phred33 -threads 16 \
        /data/duann2/human_proj/fastq/${f}_1.fastq.gz /data/duann2/human_proj/fastq/${f}_2.fastq.gz \
        /data/duann2/human_proj/repair_fastq/${f}_1_paired.fq.gz /data/duann2/human_proj/repair_fastq/${f}_1_unpaired.fq.gz \
        /data/duann2/human_proj/repair_fastq/${f}_2_paired.fq.gz /data/duann2/human_proj/repair_fastq/${f}_2_unpaired.fq.gz \
        ILLUMINACLIP:/usr/local/apps/trimmomatic/Trimmomatic-0.36/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 \
        SLIDINGWINDOW:4:15

EOT
sbatch /data/duann2/human_proj/repair_fastq/$f\.qsub
done

4. human tsv reads file generated, we need to seperate the file to different chromosome file
##################human tsv file seperate##############
chr_sep.sh
#!/bin/bash
#SBATCH --partition=norm
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --mem=200g
#SBATCH --gres=lscratch:500
#SBATCH --time=5-00:00:00

for i in `cat /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/tsv_file/good.txt`
do
        for f in {1..22}
        do
        #mkdir /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/tsv_file/chr$f
        grep -E -w ''chr$f'' /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/${i}_sortedIndexed.bam.tsv >> /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/tsv_file/chr$f/${i}_chr${f}\_ab.tsv
        done
done
# you need to do extra two steps for x and y chromosome

5. extract unmapped reads from sam file to see if it can map to the microbes
unmap.sh
!/bin/bash
for i in `cat /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/srr95.txt`
do
cat <<EOT >> $i\.qsub
#!/bin/bash
#SBATCH --partition=norm
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --mem=200g
#SBATCH --gres=lscratch:500
#SBATCH --time=5-00:00:00
module load bedtools
module load samtools

samtools view -b -f 4 /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/bamfile/${i}_sortedIndexed.bam > /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/unmapped/${i}_unmapped.bam

#samtools sort /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/unmapped/${i}_unmapped.bam -o /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/unmapped/${i}_unmapped.qsort.bam

#bedtools bamtofastq -i /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/unmapped/${i}_unmapped.qsort.bam -fq /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/unmap_fq2/${i}.unmapped.end1.fastq -fq2 /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/unmap_fq2/${i}.unmapped.end2.fastq 
#bedtools bamtofastq -i unmapped.bam -fq $i.unmapped.fastq #retrieve single fastq works
!!!I can't extract the two fastq file from bam, so I use the single one to mapping in human project in the next step

EOT
sbatch /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/$i\.qsub
done

#we don't need this step because the redistribute step in shogun: 6.1 use single fastq mapped to the indexed rep82 database obtained from shogun pipeline
https://github.com/knights-lab/SHOGUN. #shogun doesn't work which create the empty file. so I am manually run it
wget -i https://raw.githubusercontent.com/knights-lab/SHOGUN/master/docs/shogun_db_links.txt
#!/bin/bash
cd /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/qsubfile/
for f in `cat srr95.txt`
do
cat <<EOT >> $f\.qsub
#!/bin/bash
#SBATCH --partition=norm
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16 
#SBATCH --mem=32g
#SBATCH --gres=lscratch:800
#SBATCH --time=10-00:00:00
module load bowtie/2
cd /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/db/bt2/

bowtie2 -x rep82 -p 16 -f -U /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/unmap_fq/${f}.unmapped.fasta -S /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/bowtie_sam/${f}.unmapped.sam

EOT
sbatch /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/qsubfile/$f\.qsub
done


6.2 I am using burst aligner (https://github.com/knights-lab/BURST) to align the reads to rep82 database. 
!!! You are gonna need a greater mem for the burst. If you don't have enough mem you will have "bus error"
!!! sinteractive --cpus-per-task=16 --mem=300g 
conda create burst
conda install -c bioconda burst
burst_linux_DB15 -q SRR13215398.unmapped.fasta -a burst/rep82.acx -r burst/rep82.edx -b rep82.tax -o output.txt 
output format:http://www.drive5.com/usearch/manual/blast6out.html
Field:
1	 	Query label.
2	 	Target (database sequence or cluster centroid) label.
3	 	Percent identity.
4	 	Alignment length.
5	 	Number of mismatches.
6	 	Number of gap opens.
7	 	Start position in query. Query coordinates start with 1 at the first base in the sequence as it appears in the input file. For translated searches (nucleotide queries, protein targets), query start<end for +ve frame and start>end for -ve frame.
8	 	End position in query.
9	 	Start position in target. Target coordinates start with 1 at the first base in sequence as it appears in the database. For untranslated nucleotide searches, target start<end for plus strand, start>end for a reverse-complement alignment.
10	 	End position in target.
11	 	E-value calculated using Karlin-Altschul statistics.
12	 	Bit score calculated using Karlin-Altschul statistics.
Columns 11 and 12 instead refer to total edit distance (number of differences between query and reference in total) and 
whether the query is an exact duplicate of the query above it (1 if so), respectively. 
If taxonomy is assigned (-m CAPITALIST -b taxonomy.txt), that particular read's (interpolated if CAPITALIST) taxonomy is reported in column 13.

conda activate shogun
shogun filter -i SRR13215398.unmapped.fasta -d . -o filter_test/ -t 16
shogun align -a burst -i SRR13215398.unmapped.fasta -d . -o output_test/ -t 16
shogun assign_taxonomy -a burst -i output_test/alignment.burst.b6 -d . -o coverage.txt 
shogun coverage -i output_test/alignment.burst.b6 -d . -o shoguncoverage_strain.txt -l strain 
shogun redistribute -i coverage.txt -d . -l all -o redistribute.txt
shogun normalize -i coverage.txt -o normalized.txt

7. I am running shogun for 95 samples
shogunfilter.sh
#!/bin/bash
cd /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/qsubfile
for f in `cat srr95.txt`
do
cat <<EOT >> $f\.qsub
#!/bin/bash
#SBATCH --partition=norm
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16 
#SBATCH --mem=300g
#SBATCH --gres=lscratch:800
#SBATCH --time=10-00:00:00

export PATH="/data/duann2/deeplearning/conda/envs/shogun/bin:$PATH"
conda activate shogun
shogun filter -i /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/unmap_fq/${f}.unmapped.fasta -d /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/db/ -o /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/shogun_filter/$f -t 16
EOT
sbatch /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/qsubfile/$f\.qsub
done

#shogun align -a burst -i /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/unmap_fq/${i}.unmapped.fasta -d /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/ -o shogun_align/${i}/ -t 16
#shogun assign_taxonomy -a burst -i shogun_align/${i}/alignment.burst.b6 -d . -o shogun_taxonomy/${f}_coverage.txt
#shogun coverage -i shogun_align/${i}/alignment.burst.b6 -d . -o shoguncoverage_strain.txt -l strain
#shogun redistribute -i coverage.txt -d . -l all -o redistribute.txt
#shogun normalize -i coverage.txt -o normalized.txt

8. I realized that all the unmapped reads are singleton, so I use samtools to extract the fasta from unmapped sam
ref:https://manpages.ubuntu.com/manpages/focal/man1/samtools-fasta.1.html
samtool.unmap.sh
#!/bin/bash
cd /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/qsubfile/
for i in `cat srr95.txt`
do
cat <<EOT >> $i\.qsub
#!/bin/bash
#SBATCH --partition=norm
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --mem=200g
#SBATCH --gres=lscratch:500
#SBATCH --time=5-00:00:00
module load samtools

#samtools fasta -1 ${i}_pair1.fa -2 ${i}_pair2.fa -0 /dev/null -s /dev/null -n /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/unmapped/${i}_unmapped.bam 

samtools fasta -0 /dev/null /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/unmapped/${i}_unmapped.bam > ${i}_all.unmapped.fasta 

EOT
sbatch /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/qsubfile/$i\.qsub
done

###I realized that the micrbiome reads contain polyG so, I am going to trim it ####
ref:https://github.com/knights-lab/shi7
wget https://github.com/knights-lab/shi7/releases/download/v0.9.9/shi7_0.9.9_linux_release.zip
unzip shi7_*_release.zip
echo "PATH=$PWD/shi7_0.9.9_linux_release:$PATH" >> ~/.bashrc
. ~/.bash_profile
. ~/.bashrc
shi7.py -h
##I end it up to use the fastp
fastp.sh
#!/bin/bash
cd /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/qsubfile/
for i in `cat srr95.txt`
do
cat <<EOT >> $i\.qsub
#!/bin/bash
#SBATCH --partition=norm
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=50g
#SBATCH --gres=lscratch:500
#SBATCH --time=5-00:00:00
module load fastp
fastp -x -g -i /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/unmap_fq/${i}.unmapped.fastq -o /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/fastp/${i}.unmapped.trim.fastq

EOT
sbatch /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/qsubfile/$i\.qsub
done

##### shogun pipeline ####
shogunfilter.sh
#!/bin/bash
cd /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/qsubfile
for f in `cat srr95.txt`
do
cat <<EOT >> $f\.qsub
#!/bin/bash
#SBATCH --partition=norm
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16 
#SBATCH --mem=300g
#SBATCH --gres=lscratch:800
#SBATCH --time=10-00:00:00

export PATH="/data/duann2/deeplearning/conda/envs/shogun/bin:$PATH"
conda activate shogun
shogun filter -i /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/fastp/${f}.unmapped.trim.fasta -d /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/db/ -o /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/shogun_filter/$f -t 16
EOT
sbatch /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/qsubfile/$f\.qsub
done

### run shogun align test using bowtie2 ####
#chnage the title format using header change
shogun align -a bowtie2 -i srr5152newhead.fna -d /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/db/ -o . -t 8
shogun assign_taxonomy -a bowtie2 -i alignment.bowtie2.sam -d /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/db/ -o assign_tax.txt
shogun redistribute -i assign_tax.txt -d /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/db/ -l all -o redistribute.txt
shogun normalize -i assign_tax.txt -o normalized.txt


bowtie2 -x /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/db/bt2/rep82 -p 16 -f -U SRR13215352.new.fna -S /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/align_test/bowtiemap/SRR13215352.unmap.sam
samtools view -bS -@ 8 SRR13215351.unmap.sam | samtools sort > SRR13215351.unmap.sortedIndexed.bam
samtools merge combine.bam SRR13215351.unmap.sortedIndexed.bam SRR13215352.unmap.sortedIndexed.bam
samtools view -b -F 4 combine.bam > combine.mapped.bam
samtools view -o combine.mapped.sam combine.mapped.bam
shogun assign_taxonomy -a bowtie2 -i combine.mapped.sam -d /data/EmiolaLab/duann2/human_proj/human_ref/samfile/done_map/bam_file/extract_unmap/shogun_pip/db/ -o assign_tax.txt

import sys,getopt
def main(argv):
   inputfile = ''
   outputfile = ''
   try:
      opts, args = getopt.getopt(argv,"hi:o:",["ifile=","ofile="])
   except getopt.GetoptError:
      print 'test.py -i <inputfile> -o <outputfile>'
      sys.exit(2)
   for opt, arg in opts:
      if opt == '-h':
         print 'test.py -i <inputfile> -o <outputfile>'
         sys.exit()
      elif opt in ("-i", "--ifile"):
         inputfile = arg
      elif opt in ("-o", "--ofile"):
         outputfile = arg
df = pd.read_csv(inputfile,header=None,index_col=False)
df=df.replace('\.','_',regex=True)
df.to_csv(outputfile, sep = '\t',header=False,index=False)

if __name__ == "__main__":
   main(sys.argv[1:])

#https://www.biostars.org/p/56246/
Flag        Chr     Description
0x0001      p       the read is paired in sequencing
0x0002      P       the read is mapped in a proper pair
0x0004      u       the query sequence itself is unmapped
0x0008      U       the mate is unmapped
0x0010      r       strand of the query (1 for reverse)
0x0020      R       strand of the mate
0x0040      1       the read is the first read in a pair
0x0080      2       the read is the second read in a pair
0x0100      s       the alignment is not primary
0x0200      f       the read fails platform/vendor quality checks
0x0400      d       the read is either a PCR or an optical duplicate
